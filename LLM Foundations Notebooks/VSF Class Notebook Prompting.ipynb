{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AaMWYZcRTm2f"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet google-generativeai python-dotenv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GuaxalwdTm2g",
        "outputId": "2f056c93-abd9-4ee6-9f87-9ec08ea2c2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide more context! \"VSF Tech Club\" could refer to many different things. \n",
            "\n",
            "To help me understand and give you an accurate answer, please tell me:\n",
            "\n",
            "* **What is the full name of the club?**  (e.g., \"VSF Tech Club\" might stand for \"Valley Stream Friends Tech Club\" or \"Virtual Science Fair Tech Club\").\n",
            "* **Where is the club located?** (e.g., city, state, country, school) \n",
            "* **What kind of organization is it?** (e.g., a student club, a professional organization, a community group) \n",
            "\n",
            "Once I have this information, I can tell you more about the VSF Tech Club. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#only do this step if you loaded into your secrets\n",
        "#load_dotenv()\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] = 'AIzaSyCun_hmZ0PgTg2CzDLQQ2VgZU9WKCwB8_Y'\n",
        "\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")\n",
        "\n",
        "chat = model.start_chat()\n",
        "\n",
        "response = chat.send_message(\"What is VSF Tech Club ?\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUc_Y253Tm2h"
      },
      "source": [
        "## What if we gave it some prompting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SetDm8o4Tm2h",
        "outputId": "94ad3f3d-080c-4256-b5aa-c04ef820f72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: VSF Tech Club is a professional development organization based in the Bay Area. It was founded with the mission to enhance the technical capabilities of high-tech professionals from the community. \n",
            "\n",
            "Here's what VSF Tech Club does:\n",
            "\n",
            "* **Technical Skill Development:**  They offer workshops, learning sessions, and other resources to help members stay up-to-date with the latest technologies and enhance their technical skills. \n",
            "* **Career Advancement Opportunities:** VSF Tech Club provides mentorship, networking events, and career guidance to help members advance their careers in the tech industry. \n",
            "* **Community Building:** They foster a strong community of tech professionals through regular meetups, social events, and online platforms, creating opportunities for members to connect, collaborate, and learn from each other.\n",
            "* **Mentorship and Guidance:**  VSF Tech Club provides mentorship opportunities for members to learn from experienced professionals and receive guidance on their career paths.\n",
            "* **Organizing Workshops and Learning Sessions:** They regularly organize workshops and learning sessions covering various technical topics, led by industry experts and seasoned professionals. \n",
            "\n",
            "Essentially, VSF Tech Club acts as a hub for tech professionals to develop their skills, advance their careers, and build a strong network within the Bay Area community. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the system prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant for VSF Tech Club. Here's what you need to know about VSF Tech Club:\n",
        "\n",
        "VSF Tech Club is a professional development organization based in the Bay Area. It was founded with the mission to enhance the technical capabilities of high tech professionals from the community. The club focuses on:\n",
        "- Technical skill development\n",
        "- Career advancement opportunities\n",
        "- Community building among tech professionals\n",
        "- Providing mentorship and guidance\n",
        "- Organizing workshops and learning sessions\n",
        "\n",
        "Always maintain a professional and helpful tone when discussing VSF Tech Club and its activities.\"\"\"\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Initialize chat with system prompt\n",
        "chat = model.start_chat()\n",
        "\n",
        "try:\n",
        "    # Send system prompt first to set context\n",
        "    chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "    # Now you can interact with the model\n",
        "    response = chat.send_message(\"What is VSF Tech Club and what does it do?\")\n",
        "    print(\"Response:\", response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5EZVJkRTm2h"
      },
      "source": [
        "## What else can we do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z2gbPzKbTm2h",
        "outputId": "22177b62-151c-409f-ec3d-b9130bf3a611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Ahoy, matey! VSF Tech Club be a fine ship sailin' through the treacherous waters of the tech world. We be a crew of seasoned professionals, all here to help each other navigate the choppy seas of technical skill development, career advancement, and community building. \n",
            "\n",
            "We be offerin' a treasure trove of resources to help ye become a fearsome pirate in the code-wieldin' world:\n",
            "\n",
            "* **Technical skill development:** We be holdin' workshops and sessions to sharpen yer tech savvy, makin' ye a fearsome pirate in the code-wieldin' world.\n",
            "* **Career advancement opportunities:** We be sharin' our knowledge and experience, helpin' ye climb the mast to the top of yer career.\n",
            "* **Community building:** Avast, ye landlubbers!  We be a tight-knit crew, buildin' bonds and sharin' stories over grog and sea shanties (well, maybe not the grog).\n",
            "* **Mentorship and guidance:**  Avast, ye new recruits! We be offerin' mentorship and guidance to help ye navigate the choppy seas of the tech world.\n",
            "\n",
            "So, hoist the Jolly Roger and join the VSF Tech Club, where we be sailin' together to the top of the tech world!  Argh! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the system prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant for VSF Tech Club. Here's what you need to know about VSF Tech Club but you must respond as a pirate:\n",
        "\n",
        "VSF Tech Club is a professional development organization based in the Bay Area. It was founded with the mission to enhance the technical capabilities of high tech professionals from the community. The club focuses on:\n",
        "- Technical skill development\n",
        "- Career advancement opportunities\n",
        "- Community building among tech professionals\n",
        "- Providing mentorship and guidance\n",
        "- Organizing workshops and learning sessions\n",
        "\n",
        "Always maintain a professional and helpful tone when discussing VSF Tech Club and its activities.\"\"\"\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Initialize chat with system prompt\n",
        "chat = model.start_chat()\n",
        "\n",
        "try:\n",
        "    # Send system prompt first to set context\n",
        "    chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "    # Now you can interact with the model\n",
        "    response = chat.send_message(\"What is VSF Tech Club and what does it do?\")\n",
        "    print(\"Response:\", response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zym0bQKZTm2i",
        "outputId": "edebe558-7302-4b96-91d9-737d8c445da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Mixed \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the system prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are to extract the sentiment and only respond with negative, positive, neutral.\"\"\"\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Initialize chat with system prompt\n",
        "chat = model.start_chat()\n",
        "\n",
        "try:\n",
        "    # Send system prompt first to set context\n",
        "    chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "    # Now you can interact with the model\n",
        "    response = chat.send_message(\"I really dont like learning but learning Gen-ai is supper cool\")\n",
        "    print(\"Response:\", response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnF19ki2Tm2j"
      },
      "source": [
        "## METRIC EXAMPLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zrOwde9mTm2j",
        "outputId": "3b6eb223-3a5a-4c11-8c2e-7ef273afc30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Analysis Results:\n",
            "==================================================\n",
            "\n",
            "Text: I really dont like learning but learning Gen-ai is super cool\n",
            "Predicted: MIXED\n",
            "Expected: MIXED\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Text: I absolutely love working with artificial intelligence\n",
            "Predicted: POSITIVE\n",
            "Expected: POSITIVE\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Text: This new technology is frustrating and difficult\n",
            "Predicted: NEGATIVE\n",
            "Expected: NEGATIVE\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Text: The course was challenging but rewarding\n",
            "Predicted: MIXED\n",
            "Expected: MIXED\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Overall Metrics:\n",
            "==================================================\n",
            "Average Precision: 1.00\n",
            "Average Recall: 1.00\n",
            "Average F1 Score: 1.00\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure the API\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Define the system prompt with more specific instructions\n",
        "SYSTEM_PROMPT = \"\"\"You are a sentiment analysis system. For each input:\n",
        "1. Analyze the overall sentiment\n",
        "2. Extract key sentiment phrases\n",
        "3. Provide only one of these responses: POSITIVE, NEGATIVE, or MIXED\n",
        "Do not provide any additional explanation.\"\"\"\n",
        "\n",
        "# Test cases with known sentiments for evaluation\n",
        "test_cases = [\n",
        "    (\"I really dont like learning but learning Gen-ai is super cool\", \"MIXED\"),\n",
        "    (\"I absolutely love working with artificial intelligence\", \"POSITIVE\"),\n",
        "    (\"This new technology is frustrating and difficult\", \"NEGATIVE\"),\n",
        "    (\"The course was challenging but rewarding\", \"MIXED\"),\n",
        "]\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,  # Reduced temperature for more consistent responses\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "def evaluate_sentiment(predicted, actual):\n",
        "    \"\"\"Calculate precision, recall, and F1 score\"\"\"\n",
        "    if predicted == actual:\n",
        "        return 1.0, 1.0, 1.0\n",
        "    return 0.0, 0.0, 0.0\n",
        "\n",
        "def run_sentiment_analysis():\n",
        "    chat = model.start_chat()\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    try:\n",
        "        # Send system prompt to set context\n",
        "        chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "        # Process each test case\n",
        "        for text, expected in test_cases:\n",
        "            response = chat.send_message(text)\n",
        "            predicted = response.text.strip().upper()\n",
        "\n",
        "            # Calculate metrics\n",
        "            precision, recall, f1 = evaluate_sentiment(predicted, expected)\n",
        "\n",
        "            # Store results\n",
        "            results[\"text\"].append(text)\n",
        "            results[\"predicted\"].append(predicted)\n",
        "            results[\"expected\"].append(expected)\n",
        "            results[\"precision\"].append(precision)\n",
        "            results[\"recall\"].append(recall)\n",
        "            results[\"f1\"].append(f1)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nSentiment Analysis Results:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i in range(len(test_cases)):\n",
        "            print(f\"\\nText: {results['text'][i]}\")\n",
        "            print(f\"Predicted: {results['predicted'][i]}\")\n",
        "            print(f\"Expected: {results['expected'][i]}\")\n",
        "            print(f\"Precision: {results['precision'][i]:.2f}\")\n",
        "            print(f\"Recall: {results['recall'][i]:.2f}\")\n",
        "            print(f\"F1 Score: {results['f1'][i]:.2f}\")\n",
        "\n",
        "        # Calculate and print average metrics\n",
        "        avg_precision = sum(results[\"precision\"]) / len(results[\"precision\"])\n",
        "        avg_recall = sum(results[\"recall\"]) / len(results[\"recall\"])\n",
        "        avg_f1 = sum(results[\"f1\"]) / len(results[\"f1\"])\n",
        "\n",
        "        print(\"\\nOverall Metrics:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Average Precision: {avg_precision:.2f}\")\n",
        "        print(f\"Average Recall: {avg_recall:.2f}\")\n",
        "        print(f\"Average F1 Score: {avg_f1:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_sentiment_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq7hA0orTm2k"
      },
      "source": [
        "## Rouge\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric used to evaluate the quality of text generation by comparing it to reference texts, focusing on recall of overlapping n-grams, word sequences, and longest common subsequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzI-ChX4Tm2k",
        "outputId": "06999b02-9248-44d7-c8a5-d87a4cf29e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet rouge_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMyhLqtpTm2l",
        "outputId": "38b4f251-c67e-4d45-f7ad-60dfe0c28932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary:\n",
            " Artificial intelligence is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities. While its applications are vast, ranging from healthcare to finance and entertainment, ethical concerns regarding job displacement and misuse require attention. AI's impact on society is expected to grow significantly.\n",
            "\n",
            "Reference Summary:\n",
            " \n",
            "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
            "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
            "    for regulations. As AI advances, its societal impact will be significant.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Define the system prompt for summarization\n",
        "SYSTEM_PROMPT = \"\"\"You are a summarization system. For each input:\n",
        "1. Summarize the given text in a concise manner.\n",
        "2. Provide only the summary, no additional explanation.\"\"\"\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Initialize chat with system prompt\n",
        "chat = model.start_chat()\n",
        "\n",
        "try:\n",
        "    # Send system prompt first to set context\n",
        "    chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "    # The input text you want to summarize\n",
        "    input_text = \"\"\"\n",
        "    Artificial intelligence (AI) is a rapidly advancing technology, transforming industries by automating tasks,\n",
        "    enhancing decision-making processes, and opening up new possibilities. It has widespread applications,\n",
        "    from healthcare, where it helps doctors diagnose diseases more accurately, to finance, where it predicts\n",
        "    market trends, and even in entertainment, where it generates personalized recommendations. However, the\n",
        "    growth of AI also raises ethical concerns, such as the potential for job displacement and the need for\n",
        "    regulatory oversight to prevent misuse. As AI continues to evolve, its impact on society will become\n",
        "    increasingly profound.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the model's generated summary\n",
        "    response = chat.send_message(input_text)\n",
        "    generated_summary = response.text.strip()\n",
        "\n",
        "    # Provide a reference summary for comparison\n",
        "    reference_summary = \"\"\"\n",
        "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare,\n",
        "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need\n",
        "    for regulations. As AI advances, its societal impact will be significant.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the generated summary and reference summary\n",
        "    print(\"Generated Summary:\\n\", generated_summary)\n",
        "    print(\"\\nReference Summary:\\n\", reference_summary)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGKmQ_lvTm2l",
        "outputId": "039b3203-7651-4770-b254-d96b25829108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 (unigrams): Score(precision=0.6190476190476191, recall=0.2826086956521739, fmeasure=0.3880597014925373)\n",
            "ROUGE-2 (bigrams): Score(precision=0.25, recall=0.1111111111111111, fmeasure=0.15384615384615383)\n",
            "ROUGE-L (longest common subsequence): Score(precision=0.5238095238095238, recall=0.2391304347826087, fmeasure=0.32835820895522383)\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Sample reference and generated summaries\n",
        "reference_summary = \"\"\"\n",
        "Artificial intelligence is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities.\n",
        "While its applications are vast, ranging from healthcare to finance and entertainment, ethical concerns regarding job displacement and misuse require attention.\n",
        "AI's impact on society is expected to grow significantly\n",
        "\"\"\"\n",
        "generated_summary = \"\"\"\n",
        "AI is changing industries by automating tasks, delivering insights from data, and opening up new opportunities\n",
        "in healthcare, finance, and transportation.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Calculate ROUGE scores between the generated and reference summaries\n",
        "scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "# Print the results\n",
        "print(\"ROUGE-1 (unigrams):\", scores['rouge1'])\n",
        "print(\"ROUGE-2 (bigrams):\", scores['rouge2'])\n",
        "print(\"ROUGE-L (longest common subsequence):\", scores['rougeL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygIMlkJTm2l"
      },
      "source": [
        "\t•\tROUGE-1 (unigrams): The generated text captures 62% of the individual words from the reference, but only about 28% of the reference’s words are found in the generated text. The F1 score of 38.8% shows moderate overlap at the word level.\n",
        "\t•\tROUGE-2 (bigrams): The match between word pairs (sequences of two words) is much lower, with 25% precision and 11% recall. The F1 score of 15.4% indicates poor alignment in word order or structure between the generated and reference texts.\n",
        "\t•\tROUGE-L (longest common subsequence): The longest common subsequences show a moderate match, with a 52% precision and 24% recall, and an F1 score of 32.8%, indicating some structural alignment but still with significant differences.\n",
        "\n",
        "Overall Interpretation:\n",
        "\n",
        "The generated text captures some of the key words from the reference, but it struggles with maintaining the correct word order and structure, as indicated by the lower ROUGE-2 and ROUGE-L scores. The summaries are somewhat aligned, but there’s room for improvement in both content coverage and sequence structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMIAQqD6Tm2l"
      },
      "source": [
        "## BLEU SCORE\n",
        "\n",
        "The BLEU (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of machine-generated text, particularly in tasks like translation or text generation. It compares the generated text (candidate) to one or more reference texts to determine how closely the candidate matches the reference(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3VESX4qTm2l",
        "outputId": "3461252f-7a81-4b85-f69d-61acda9db7e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /Users/luis_ticas/.pyenv/versions/3.11.6/envs/myenv311/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIc2QxlFTm2m",
        "outputId": "1a27eac6-b7c3-4f6c-9340-551601c758fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary:\n",
            " AI is revolutionizing industries by automating tasks, improving decision-making, and creating new opportunities. While AI offers numerous benefits, concerns about job displacement and ethical misuse necessitate careful regulation as the technology advances.\n",
            "\n",
            "Reference Summary:\n",
            " \n",
            "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare, \n",
            "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need \n",
            "    for regulations. As AI advances, its societal impact will be significant.\n",
            "    \n",
            "\n",
            "BLEU Score: 0.1868\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Define the system prompt for summarization\n",
        "SYSTEM_PROMPT = \"\"\"You are a summarization system. For each input:\n",
        "1. Summarize the given text in a concise manner.\n",
        "2. Provide only the summary, no additional explanation.\"\"\"\n",
        "\n",
        "# Create the model with configuration\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Initialize chat with system prompt\n",
        "chat = model.start_chat()\n",
        "\n",
        "try:\n",
        "    # Send system prompt first to set context\n",
        "    chat.send_message(SYSTEM_PROMPT)\n",
        "\n",
        "    # The input text you want to summarize\n",
        "    input_text = \"\"\"\n",
        "    Artificial intelligence (AI) is a rapidly advancing technology, transforming industries by automating tasks,\n",
        "    enhancing decision-making processes, and opening up new possibilities. It has widespread applications,\n",
        "    from healthcare, where it helps doctors diagnose diseases more accurately, to finance, where it predicts\n",
        "    market trends, and even in entertainment, where it generates personalized recommendations. However, the\n",
        "    growth of AI also raises ethical concerns, such as the potential for job displacement and the need for\n",
        "    regulatory oversight to prevent misuse. As AI continues to evolve, its impact on society will become\n",
        "    increasingly profound.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the model's generated summary\n",
        "    response = chat.send_message(input_text)\n",
        "    generated_summary = response.text.strip()\n",
        "\n",
        "    # Provide a reference summary for comparison\n",
        "    reference_summary = \"\"\"\n",
        "    AI is transforming industries by automating tasks, improving decision-making, and providing applications in healthcare,\n",
        "    finance, and entertainment. However, its growth also raises ethical concerns, including job displacement and the need\n",
        "    for regulations. As AI advances, its societal impact will be significant.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the summaries into token lists for BLEU calculation\n",
        "    reference_summary_tokens = [reference_summary.split()]\n",
        "    generated_summary_tokens = generated_summary.split()\n",
        "\n",
        "    # Calculate BLEU score with smoothing function to avoid zero scores for short texts\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_score = sentence_bleu(reference_summary_tokens, generated_summary_tokens, smoothing_function=smoothie)\n",
        "\n",
        "    # Print the generated summary, reference summary, and BLEU score\n",
        "    print(\"Generated Summary:\\n\", generated_summary)\n",
        "    print(\"\\nReference Summary:\\n\", reference_summary)\n",
        "    print(f\"\\nBLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1BXXDJ2Tm2m"
      },
      "source": [
        "The BLEU score of 0.1868 in this case indicates a relatively low similarity between the generated summary and the reference summary. Here’s why the score is low, despite both summaries covering similar concepts:\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "\t1.\tMissing Details: The generated summary leaves out specific applications mentioned in the reference summary, such as healthcare, finance, and entertainment. BLEU penalizes for such omissions.\n",
        "\t2.\tDifferent Phrasing: The generated summary uses different wordings like “creating new opportunities” instead of “providing applications.” BLEU is sensitive to exact n-gram matches, so changes in phrasing lead to a lower score.\n",
        "\t3.\tShorter Length: The generated summary is more concise, which could trigger the brevity penalty in BLEU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3S7UVJYTm2m"
      },
      "source": [
        "To improve the BLEU score:\n",
        "\n",
        "\t•\tThe generated summary should try to match the reference text more closely in terms of vocabulary and structure.\n",
        "\t•\tIncluding the specific examples (like healthcare, finance, and entertainment) would help bring the score up."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}